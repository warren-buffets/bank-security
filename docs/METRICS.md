# M√©triques et KPI - FraudGuard

## Vue d'ensemble

Ce document d√©finit les m√©triques cl√©s pour √©valuer et monitorer le syst√®me de d√©tection de fraude FraudGuard. Ces m√©triques couvrent √† la fois la **performance ML** et l'**impact business**.

---

## üìä M√©triques ML (Model Performance)

### 1. AUC-ROC (Area Under the Receiver Operating Characteristic Curve)

**D√©finition** : Mesure la capacit√© du mod√®le √† distinguer entre transactions frauduleuses et l√©gitimes, ind√©pendamment du seuil de d√©cision.

**Formule** :
```
AUC = ‚à´‚ÇÄ¬π TPR(FPR) d(FPR)
o√π TPR = True Positive Rate (Recall)
    FPR = False Positive Rate
```

**Interpr√©tation** :
- **AUC = 1.0** : Mod√®le parfait (s√©pare compl√®tement fraude/non-fraude)
- **AUC = 0.9-0.95** : Excellent
- **AUC = 0.8-0.9** : Bon
- **AUC = 0.7-0.8** : Acceptable
- **AUC < 0.7** : M√©diocre
- **AUC = 0.5** : √âquivalent √† un tirage al√©atoire

**Objectif FraudGuard** :
- ‚úÖ **Minimum acceptable** : AUC ‚â• 0.90
- üéØ **Objectif** : AUC ‚â• 0.94
- üöÄ **Excellence** : AUC ‚â• 0.96

**Pourquoi c'est important** :
- M√©trique standard de l'industrie pour comparer les mod√®les
- Ind√©pendante du seuil de d√©cision (threshold-agnostic)
- Refl√®te la qualit√© intrins√®que du scoring

**Monitoring** :
```python
# Calculer l'AUC en production
from sklearn.metrics import roc_auc_score

auc = roc_auc_score(y_true, y_pred_proba)

# Alerte si AUC < 0.90 (drift d√©tect√©)
if auc < 0.90:
    alert_model_degradation()
```

---

### 2. FPR (False Positive Rate) - Taux de Faux Positifs

**D√©finition** : Proportion de transactions l√©gitimes incorrectement class√©es comme frauduleuses.

**Formule** :
```
FPR = FP / (FP + TN)
o√π FP = False Positives (vraies transactions bloqu√©es)
    TN = True Negatives (vraies transactions autoris√©es)
```

**Interpr√©tation** :
- **FPR = 0%** : Aucun faux positif (id√©al mais irr√©aliste)
- **FPR = 1-2%** : Excellent (friction minimale)
- **FPR = 3-5%** : Acceptable
- **FPR > 5%** : Probl√©matique (frustration client)

**Objectif FraudGuard** :
- üéØ **Objectif** : FPR < 2%
- ‚ö†Ô∏è **Alerte** : FPR > 3%
- üö® **Critique** : FPR > 5%

**Impact Business** :
```
FPR de 2% sur 1M transactions/jour = 20,000 clients l√©gitimes bloqu√©s
‚Üí Perte de revenus potentielle
‚Üí Insatisfaction client
‚Üí Appels au support
```

**Trade-off** : FPR vs TPR (True Positive Rate / Recall)
- Baisser le seuil ‚Üí ‚Üë TPR (d√©tecte plus de fraudes) mais ‚Üë FPR (plus de faux positifs)
- Augmenter le seuil ‚Üí ‚Üì FPR (moins de faux positifs) mais ‚Üì TPR (manque des fraudes)

**Monitoring** :
```python
# Calculer FPR par segment
fpr_per_country = calculate_fpr_by_segment(transactions, 'country')

# Alerte si FPR d'un segment > 5%
for country, fpr in fpr_per_country.items():
    if fpr > 0.05:
        alert_high_fpr(country, fpr)
```

**Optimisation** :
- Ajuster le threshold par pays/segment
- Utiliser le mode **CHALLENGE** (2FA) au lieu de **DENY** pour les zones grises
- Impl√©menter un feedback loop (cas d'appels clients = faux positifs)

---

### 3. Calibration du Mod√®le

**D√©finition** : Mesure dans quelle mesure les scores pr√©dits correspondent aux probabilit√©s r√©elles.

**Objectif** : Un mod√®le bien calibr√© pr√©dit 0.8 ‚Üí 80% de chance r√©elle de fraude.

**Pourquoi c'est crucial** :
- Permet d'utiliser les scores comme **probabilit√©s business**
- Essentiel pour fixer des seuils de d√©cision rationnels
- Facilite l'interpr√©tation pour les analystes

**Test de calibration** : Courbe de fiabilit√© (Reliability Diagram)

```
Bins de score   | % fraude pr√©dit | % fraude r√©el | Calibration
----------------|-----------------|---------------|-------------
[0.0 - 0.1]    | 5%              | 4%            | ‚úÖ Bon
[0.1 - 0.2]    | 15%             | 13%           | ‚úÖ Bon
[0.2 - 0.3]    | 25%             | 27%           | ‚úÖ Bon
[0.8 - 0.9]    | 85%             | 82%           | ‚úÖ Bon
[0.9 - 1.0]    | 95%             | 93%           | ‚úÖ Bon
```

**M√©trique** : Brier Score
```
Brier Score = (1/N) Œ£ (p_i - y_i)¬≤
o√π p_i = probabilit√© pr√©dite
    y_i = vrai label (0 ou 1)

Brier Score parfait = 0
```

**Objectif FraudGuard** :
- üéØ Brier Score < 0.10

**M√©thodes de calibration** :

#### a) Platt Scaling (Regression Logistique)
```python
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV

# Entra√Æner le mod√®le de base (LightGBM)
model = lgb.train(params, train_data)

# Calibrer avec Platt Scaling sur un validation set
calibrator = LogisticRegression()
calibrator.fit(val_scores.reshape(-1, 1), val_labels)

# Pr√©dictions calibr√©es
calibrated_scores = calibrator.predict_proba(raw_scores.reshape(-1, 1))[:, 1]
```

**Avantages** :
- ‚úÖ Simple et rapide
- ‚úÖ Fonctionne bien pour les mod√®les non-calibr√©s

**Inconv√©nients** :
- ‚ùå Assume une transformation sigmo√Øde
- ‚ùå Peut sous-performer pour les distributions non-lin√©aires

#### b) Isotonic Regression (Non-param√©trique)
```python
from sklearn.isotonic import IsotonicRegression

# Calibrer avec Isotonic Regression
calibrator = IsotonicRegression(out_of_bounds='clip')
calibrator.fit(val_scores, val_labels)

# Pr√©dictions calibr√©es
calibrated_scores = calibrator.predict(raw_scores)
```

**Avantages** :
- ‚úÖ Plus flexible (non-param√©trique)
- ‚úÖ Meilleur pour les distributions complexes

**Inconv√©nients** :
- ‚ùå Risque d'overfitting sur petits datasets
- ‚ùå Plus lent

**Notre choix recommand√©** : **Platt Scaling** pour d√©marrer, puis **Isotonic Regression** si Brier Score > 0.10

**Int√©gration dans le pipeline** :
```python
# 1. Entra√Æner le mod√®le principal
model = train_lightgbm(train_data)

# 2. Calibrer sur validation set
calibrator = train_calibrator(model, val_data)

# 3. Sauvegarder les deux
save_model(model, "gbdt_v1.bin")
save_calibrator(calibrator, "calibrator_v1.pkl")

# 4. En production
raw_score = model.predict(features)
calibrated_prob = calibrator.predict(raw_score)
```

**Monitoring** :
```python
# V√©rifier la calibration en production chaque semaine
def check_calibration(predictions, labels):
    bins = np.linspace(0, 1, 11)
    for i in range(len(bins) - 1):
        mask = (predictions >= bins[i]) & (predictions < bins[i+1])
        pred_mean = predictions[mask].mean()
        true_mean = labels[mask].mean()

        if abs(pred_mean - true_mean) > 0.10:  # D√©calibration > 10%
            alert_calibration_drift(bins[i], pred_mean, true_mean)
```

---

## üíº M√©triques Business

### 4. Precision (Pr√©cision)

**D√©finition** : Proportion de vraies fraudes parmi les transactions bloqu√©es.

**Formule** :
```
Precision = TP / (TP + FP)
```

**Interpr√©tation** :
- Precision = 90% ‚Üí 9 transactions bloqu√©es sur 10 sont de vraies fraudes

**Objectif** : Precision ‚â• 75%

---

### 5. Recall (Taux de d√©tection)

**D√©finition** : Proportion de fraudes r√©ellement d√©tect√©es.

**Formule** :
```
Recall = TP / (TP + FN)
```

**Objectif** : Recall ‚â• 94%

---

### 6. F1-Score

**D√©finition** : Moyenne harmonique de Precision et Recall.

**Formule** :
```
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**Objectif** : F1 ‚â• 0.85

---

## ‚ö° M√©triques Op√©rationnelles

### 7. Latence (P95, P99)

- **P95 < 100ms** : 95% des requ√™tes r√©pondent en moins de 100ms
- **P99 < 200ms** : 99% des requ√™tes r√©pondent en moins de 200ms

### 8. Throughput

- **10,000 TPS** (transactions par seconde) en conditions normales
- **50,000 TPS** en pic

### 9. Disponibilit√©

- **SLA : 99.95%** (< 4.38 heures de downtime/an)

---

## üìà Dashboard de Monitoring

### M√©triques √† tracker en temps r√©el

```yaml
ML Metrics:
  - AUC-ROC (rolling 7 days)
  - FPR par pays/segment
  - Calibration (Brier Score)
  - Drift detection (KL divergence)

Business Metrics:
  - Montant de fraude bloqu√© (‚Ç¨)
  - Montant de faux positifs (‚Ç¨)
  - Taux de contestation (chargeback rate)
  - ROI du syst√®me

Operational Metrics:
  - P95/P99 latency
  - Throughput (TPS)
  - Error rate
  - Redis/Kafka health
```

---

## üéØ R√©sum√© des Objectifs

| M√©trique | Objectif | Alerte | Critique |
|----------|----------|--------|----------|
| **AUC-ROC** | ‚â• 0.94 | < 0.92 | < 0.90 |
| **FPR** | < 2% | > 3% | > 5% |
| **Brier Score** | < 0.10 | > 0.12 | > 0.15 |
| **Precision** | ‚â• 75% | < 70% | < 65% |
| **Recall** | ‚â• 94% | < 92% | < 90% |
| **P95 Latency** | < 100ms | > 120ms | > 150ms |

---

## üìö R√©f√©rences

- [Scikit-learn: Probability Calibration](https://scikit-learn.org/stable/modules/calibration.html)
- [Google: Rules of Machine Learning - Rule #36 (Calibration)](https://developers.google.com/machine-learning/guides/rules-of-ml)
- [Stripe: Online Payments Fraud Detection](https://stripe.com/docs/disputes)
